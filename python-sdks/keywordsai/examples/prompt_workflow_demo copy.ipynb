{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Keywords AI Prompt Management Workflow Demo\n",
        "\n",
        "This notebook demonstrates the complete workflow for managing prompts in Keywords AI, including creation, versioning, deployment, and usage with OpenAI SDK.\n",
        "\n",
        "## What are Prompts in Keywords AI?\n",
        "\n",
        "Keywords AI prompts are reusable templates that define AI model configurations, system messages, and parameters. They enable version control, A/B testing, and centralized prompt management. Each prompt can have multiple versions with different settings, allowing you to iterate and deploy the best-performing configurations. Prompts integrate seamlessly with OpenAI SDK through Keywords AI's gateway, providing analytics and monitoring for all your AI interactions.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Keywords AI API key (`KEYWORDSAI_API_KEY`)\n",
        "- Keywords AI base URL (`KEYWORDSAI_BASE_URL`) \n",
        "- Python packages: `keywordsai`, `openai`, `python-dotenv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment variables loaded successfully\n",
            "üîë API Key: h5vo0ju3.n...2BzH\n",
            "üåê Base URL: http://localhost:8000\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Keywords AI imports\n",
        "from keywordsai.prompts.api import PromptAPI\n",
        "from keywordsai_sdk.keywordsai_types.prompt_types import Prompt, PromptVersion\n",
        "\n",
        "# Check environment variables\n",
        "api_key = os.getenv(\"KEYWORDSAI_API_KEY\")\n",
        "base_url = os.getenv(\"KEYWORDSAI_BASE_URL\")\n",
        "\n",
        "if not api_key or not base_url:\n",
        "    print(\"‚ùå Missing environment variables!\")\n",
        "    print(\"Please set KEYWORDSAI_API_KEY and KEYWORDSAI_BASE_URL in your .env file\")\n",
        "else:\n",
        "    print(\"‚úÖ Environment variables loaded successfully\")\n",
        "    print(f\"üîë API Key: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else api_key}\")\n",
        "    print(f\"üåê Base URL: {base_url}\")\n",
        "\n",
        "# Initialize the Prompt API client\n",
        "client = PromptAPI(api_key=api_key, base_url=base_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create a New Prompt\n",
        "\n",
        "Let's start by creating a new prompt that will serve as our container for different versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Creating a new prompt...\n",
            "‚úÖ Created prompt with ID: 63cc66077f3b4d349ea191d981647981\n",
            "   Name: Untitled\n",
            "   Description: \n"
          ]
        }
      ],
      "source": [
        "# Create a new prompt\n",
        "print(\"üìù Creating a new prompt...\")\n",
        "\n",
        "# Create basic prompt\n",
        "prompt = await client.acreate()\n",
        "print(f\"‚úÖ Created prompt with ID: {prompt.prompt_id}\")\n",
        "print(f\"   Name: {prompt.name}\")\n",
        "print(f\"   Description: {prompt.description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úèÔ∏è  Updating prompt metadata...\n",
            "‚úÖ Updated prompt: Customer Support Assistant\n",
            "üìã Description: AI assistant for handling customer support inquiries with professional tone\n",
            "\n",
            "üîó Prompt ID for reference: 63cc66077f3b4d349ea191d981647981\n",
            "Prompt JSON representation: {\n",
            "  \"id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "  \"name\": \"Customer Support Assistant\",\n",
            "  \"description\": \"AI assistant for handling customer support inquiries with professional tone\",\n",
            "  \"prompt_id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "  \"prompt_slug\": \"\",\n",
            "  \"full_prompt_id\": null,\n",
            "  \"current_version\": null,\n",
            "  \"live_version\": null,\n",
            "  \"prompt_versions\": null,\n",
            "  \"prompt_activities\": null,\n",
            "  \"commit_count\": 0,\n",
            "  \"starred\": false,\n",
            "  \"tags\": [],\n",
            "  \"is_example\": false,\n",
            "  \"blurred\": false\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Update the prompt with meaningful information\n",
        "print(\"\\n‚úèÔ∏è  Updating prompt metadata...\")\n",
        "updated_prompt_data = Prompt(\n",
        "    name=\"Customer Support Assistant\",\n",
        "    description=\"AI assistant for handling customer support inquiries with professional tone\",\n",
        "    prompt_id=prompt.prompt_id,  # Include the required prompt_id\n",
        ")\n",
        "\n",
        "updated_prompt = await client.aupdate(prompt.prompt_id, updated_prompt_data)\n",
        "print(f\"‚úÖ Updated prompt: {updated_prompt.name}\")\n",
        "print(f\"üìã Description: {updated_prompt.description}\")\n",
        "\n",
        "# Store the prompt ID for later use\n",
        "PROMPT_ID = updated_prompt.prompt_id\n",
        "print(f\"\\nüîó Prompt ID for reference: {PROMPT_ID}\")\n",
        "print(f\"Prompt JSON representation: {updated_prompt.model_dump_json(indent=2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Listing all prompts to showcase the list feature...\n",
            "‚úÖ Found 2 total prompts\n",
            "üìä Showing first 2 prompts:\n",
            "      Created: {\n",
            "  \"id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "  \"name\": \"Customer Support Assistant\",\n",
            "  \"description\": \"AI assistant for handling customer support inquiries with professional tone\",\n",
            "  \"prompt_id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "  \"prompt_slug\": \"\",\n",
            "  \"full_prompt_id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "  \"current_version\": null,\n",
            "  \"live_version\": null,\n",
            "  \"prompt_versions\": null,\n",
            "  \"prompt_activities\": null,\n",
            "  \"commit_count\": 0,\n",
            "  \"starred\": false,\n",
            "  \"tags\": [],\n",
            "  \"prompt_live_version_number\": null\n",
            "}\n",
            "\n",
            "      Created: {\n",
            "  \"id\": \"6caa11b48d4d440986b3eb3b96ae795e\",\n",
            "  \"name\": \"Test JSON schema\",\n",
            "  \"description\": \"\",\n",
            "  \"prompt_id\": \"6caa11b48d4d440986b3eb3b96ae795e\",\n",
            "  \"prompt_slug\": \"something\",\n",
            "  \"full_prompt_id\": \"6caa11b48d4d440986b3eb3b96ae795e\",\n",
            "  \"current_version\": {\n",
            "    \"id\": \"6caa11b48d4d440986b3eb3b96ae795e-31\",\n",
            "    \"prompt_version_id\": \"6caa11b48d4d440986b3eb3b96ae795e-31\",\n",
            "    \"description\": \"link\",\n",
            "    \"created_at\": \"2020-08-07T04:49:16.908164Z\",\n",
            "    \"updated_at\": \"2025-08-12T17:58:58.014812Z\",\n",
            "    \"version\": 31,\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": [\n",
            "          {\n",
            "            \"type\": \"text\",\n",
            "            \"text\": \"Could you echo the link for me: {{ privacy_policy_info | default(\\\"https://www.usetwine.com/privacy-policy\\\") }}\",\n",
            "            \"cache_control\": null\n",
            "          }\n",
            "        ],\n",
            "        \"name\": null,\n",
            "        \"tool_call_id\": null,\n",
            "        \"tool_calls\": null,\n",
            "        \"reasoning_content\": null,\n",
            "        \"thinking_blocks\": null\n",
            "      }\n",
            "    ],\n",
            "    \"edited_by\": {\n",
            "      \"first_name\": \"Keywords AI\",\n",
            "      \"last_name\": \"Team\",\n",
            "      \"email\": \"admin@keywordsai.co\"\n",
            "    },\n",
            "    \"model\": \"gpt-3.5-turbo\",\n",
            "    \"stream\": false,\n",
            "    \"temperature\": 0.7,\n",
            "    \"max_tokens\": 4096,\n",
            "    \"top_p\": 1.0,\n",
            "    \"frequency_penalty\": 0.0,\n",
            "    \"presence_penalty\": 0.0,\n",
            "    \"reasoning_effort\": \"low\",\n",
            "    \"variables\": {},\n",
            "    \"readonly\": false,\n",
            "    \"fallback_models\": [\n",
            "      \"None\"\n",
            "    ],\n",
            "    \"load_balance_models\": null,\n",
            "    \"tools\": null,\n",
            "    \"tool_choice\": null,\n",
            "    \"response_format\": {\n",
            "      \"type\": \"text\"\n",
            "    },\n",
            "    \"is_enforcing_response_format\": false,\n",
            "    \"prompt\": null,\n",
            "    \"parent_prompt\": \"6caa11b48d4d440986b3eb3b96ae795e\",\n",
            "    \"json_schema\": null\n",
            "  },\n",
            "  \"live_version\": {\n",
            "    \"id\": 175,\n",
            "    \"prompt_version_id\": \"6caa11b48d4d440986b3eb3b96ae795e-30\",\n",
            "    \"description\": \"link\",\n",
            "    \"created_at\": \"2020-08-07T04:49:16.908164Z\",\n",
            "    \"updated_at\": \"2025-08-12T17:58:57.969600Z\",\n",
            "    \"version\": 30,\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": [\n",
            "          {\n",
            "            \"type\": \"text\",\n",
            "            \"text\": \"Could you echo the link for me: {{ privacy_policy_info | default(\\\"https://www.usetwine.com/privacy-policy\\\") }}\",\n",
            "            \"cache_control\": null\n",
            "          }\n",
            "        ],\n",
            "        \"name\": null,\n",
            "        \"tool_call_id\": null,\n",
            "        \"tool_calls\": null,\n",
            "        \"reasoning_content\": null,\n",
            "        \"thinking_blocks\": null\n",
            "      }\n",
            "    ],\n",
            "    \"edited_by\": {\n",
            "      \"first_name\": \"Keywords AI\",\n",
            "      \"last_name\": \"Team\",\n",
            "      \"email\": \"admin@keywordsai.co\"\n",
            "    },\n",
            "    \"model\": \"gpt-3.5-turbo\",\n",
            "    \"stream\": false,\n",
            "    \"temperature\": 0.7,\n",
            "    \"max_tokens\": 4096,\n",
            "    \"top_p\": 1.0,\n",
            "    \"frequency_penalty\": 0.0,\n",
            "    \"presence_penalty\": 0.0,\n",
            "    \"reasoning_effort\": \"low\",\n",
            "    \"variables\": {},\n",
            "    \"readonly\": true,\n",
            "    \"fallback_models\": [\n",
            "      \"None\"\n",
            "    ],\n",
            "    \"load_balance_models\": null,\n",
            "    \"tools\": null,\n",
            "    \"tool_choice\": null,\n",
            "    \"response_format\": {\n",
            "      \"type\": \"text\"\n",
            "    },\n",
            "    \"is_enforcing_response_format\": false,\n",
            "    \"prompt\": 306,\n",
            "    \"parent_prompt\": \"6caa11b48d4d440986b3eb3b96ae795e\",\n",
            "    \"json_schema\": null\n",
            "  },\n",
            "  \"prompt_versions\": null,\n",
            "  \"prompt_activities\": null,\n",
            "  \"commit_count\": 30,\n",
            "  \"starred\": false,\n",
            "  \"tags\": [],\n",
            "  \"prompt_live_version_number\": 30\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List all prompts to show the newly created one\n",
        "print(\"üìã Listing all prompts to showcase the list feature...\")\n",
        "prompts_list = await client.alist(page_size=10)\n",
        "print(f\"‚úÖ Found {prompts_list.count} total prompts\")\n",
        "print(f\"üìä Showing first {len(prompts_list.results)} prompts:\")\n",
        "\n",
        "for i, p in enumerate(prompts_list.results[:5], 1):  # Show first 5\n",
        "    is_our_prompt = \"üëà Our new prompt!\" if p.prompt_id == PROMPT_ID else \"\"\n",
        "    print(f\"      Created: {p.model_dump_json(indent=2)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Browse the Keywords AI UI\n",
        "\n",
        "**Now is a good time to visit the Keywords AI platform UI!**\n",
        "\n",
        "1. Go to your Keywords AI dashboard: `https://platform.keywordsai.co`\n",
        "2. Navigate to the **Prompts** section\n",
        "3. Find the prompt you just created: \"Customer Support Assistant\"\n",
        "4. Notice that it currently has no versions - you'll need to create one!\n",
        "\n",
        "The UI provides a visual interface for managing prompts, but we'll continue with the API to show the complete programmatic workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a Prompt Version\n",
        "\n",
        "Now let's create a version of our prompt with specific configuration, messages, and model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating a prompt version...\n",
            "‚úÖ Created version 1\n",
            "   Model: gpt-4o-mini\n",
            "   Temperature: 0.7\n",
            "   Messages: 2 messages\n",
            "   Variables: {'customer_inquiry': \"Customer's question or issue\"}\n",
            "\n",
            "üîó Version 1 created successfully\n",
            "   Version detail: {\"id\":\"63cc66077f3b4d349ea191d981647981-1\",\"prompt_version_id\":\"63cc66077f3b4d349ea191d981647981-1\",\"description\":\"Initial customer support version with professional tone\",\"created_at\":\"2025-08-21T01:43:02.242817Z\",\"updated_at\":\"2025-08-21T01:43:02.310866Z\",\"version\":1,\"messages\":[{\"role\":\"system\",\"content\":\"You are a professional customer support assistant. Always be helpful, polite, and provide clear solutions to customer problems.\",\"name\":null,\"tool_call_id\":null,\"tool_calls\":null,\"reasoning_content\":null,\"thinking_blocks\":null},{\"role\":\"user\",\"content\":\"{{customer_inquiry}}\",\"name\":null,\"tool_call_id\":null,\"tool_calls\":null,\"reasoning_content\":null,\"thinking_blocks\":null}],\"edited_by\":{\"first_name\":\"Keywords AI\",\"last_name\":\"Team\",\"email\":\"admin@keywordsai.co\"},\"model\":\"gpt-4o-mini\",\"stream\":false,\"temperature\":0.7,\"max_tokens\":2048,\"top_p\":1.0,\"frequency_penalty\":0.0,\"presence_penalty\":0.0,\"reasoning_effort\":null,\"variables\":{\"customer_inquiry\":\"Customer's question or issue\"},\"readonly\":false,\"fallback_models\":null,\"load_balance_models\":null,\"tools\":null,\"tool_choice\":null,\"response_format\":{\"type\":\"text\"},\"is_enforcing_response_format\":false,\"prompt\":null,\"parent_prompt\":\"63cc66077f3b4d349ea191d981647981\",\"json_schema\":null}\n"
          ]
        }
      ],
      "source": [
        "# Create a prompt version with specific configuration\n",
        "print(\"üîß Creating a prompt version...\")\n",
        "\n",
        "version_data = PromptVersion(\n",
        "    prompt_version_id=\"version-001\",\n",
        "    description=\"Initial customer support version with professional tone\",\n",
        "    created_at=datetime.now(timezone.utc),\n",
        "    updated_at=datetime.now(timezone.utc),\n",
        "    version=1,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional customer support assistant. Always be helpful, polite, and provide clear solutions to customer problems.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"{{customer_inquiry}}\"},\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=2048,\n",
        "    top_p=1.0,\n",
        "    frequency_penalty=0.0,\n",
        "    presence_penalty=0.0,\n",
        "    variables={\"customer_inquiry\": \"Customer's question or issue\"},\n",
        "    parent_prompt=PROMPT_ID,\n",
        ")\n",
        "\n",
        "version = await client.acreate_version(PROMPT_ID, version_data)\n",
        "print(f\"‚úÖ Created version {version.version}\")\n",
        "print(f\"   Model: {version.model}\")\n",
        "print(f\"   Temperature: {version.temperature}\")\n",
        "print(f\"   Messages: {len(version.messages)} messages\")\n",
        "print(f\"   Variables: {version.variables}\")\n",
        "\n",
        "# Store version info for later use\n",
        "VERSION_1 = version.version\n",
        "print(f\"\\nüîó Version {VERSION_1} created successfully\")\n",
        "print(f\"   Version detail: {version.model_dump_json(indent=2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Update a Prompt Version\n",
        "\n",
        "Let's update our version to demonstrate how you can modify existing versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Version 1 details:\n",
            "   Description: Initial customer support version with professional tone\n",
            "   Readonly: False\n",
            "\n",
            "‚úèÔ∏è  Updating version 1...\n",
            "‚úÖ Updated version 1\n",
            "   New description: Updated: Professional customer support with improved clarity\n",
            "   New temperature: 0.6\n",
            "   New max_tokens: 2500\n"
          ]
        }
      ],
      "source": [
        "# First, let's check if the version is readonly\n",
        "retrieved_version = await client.aget_version(PROMPT_ID, VERSION_1)\n",
        "print(f\"üîç Version {retrieved_version.version} details:\")\n",
        "print(f\"   Description: {retrieved_version.description}\")\n",
        "print(f\"   Readonly: {retrieved_version.readonly}\")\n",
        "\n",
        "if retrieved_version.readonly:\n",
        "    print(f\"\\n‚ö†Ô∏è  Version {retrieved_version.version} is readonly - cannot update\")\n",
        "    print(\"   This is expected behavior - versions may become readonly after creation\")\n",
        "else:\n",
        "    print(f\"\\n‚úèÔ∏è  Updating version {VERSION_1}...\")\n",
        "    # Only send the fields we want to update, not a full PromptVersion object\n",
        "    update_version_data = {\n",
        "        \"description\": \"Updated: Professional customer support with improved clarity\",\n",
        "        \"temperature\": 0.6,  # Slightly lower temperature\n",
        "        \"max_tokens\": 2500,  # More tokens\n",
        "    }\n",
        "    \n",
        "    updated_version = await client.aupdate_version(\n",
        "        PROMPT_ID, VERSION_1, update_version_data\n",
        "    )\n",
        "    print(f\"‚úÖ Updated version {updated_version.version}\")\n",
        "    print(f\"   New description: {updated_version.description}\")\n",
        "    print(f\"   New temperature: {updated_version.temperature}\")\n",
        "    print(f\"   New max_tokens: {updated_version.max_tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup OpenAI Client with Keywords AI\n",
        "\n",
        "Now let's setup the OpenAI client to use Keywords AI as the gateway. This allows us to use our prompts with the standard OpenAI SDK.\n",
        "\n",
        "Reference: [Keywords AI OpenAI SDK Integration](https://docs.keywordsai.co/integration/development-frameworks/llm_framework/openai/openai-sdk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI client configured to use Keywords AI gateway\n",
            "üîó Gateway URL: http://localhost:8000/api/\n",
            "\n",
            "üí° This allows you to:\n",
            "   - Use Keywords AI prompts with OpenAI SDK\n",
            "   - Access 250+ models through one interface\n",
            "   - Get detailed analytics and monitoring\n",
            "   - Manage fallbacks and error handling\n"
          ]
        }
      ],
      "source": [
        "# Setup OpenAI client to use Keywords AI gateway\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client with Keywords AI endpoint\n",
        "openai_client = OpenAI(\n",
        "    base_url=f\"{base_url.rstrip('/')}/api/\",  # Keywords AI gateway endpoint\n",
        "    api_key=api_key,  # Use your Keywords AI API key\n",
        ")\n",
        "\n",
        "print(\"‚úÖ OpenAI client configured to use Keywords AI gateway\")\n",
        "print(f\"üîó Gateway URL: {base_url.rstrip('/')}/api/\")\n",
        "print(\"\\nüí° This allows you to:\")\n",
        "print(\"   - Use Keywords AI prompts with OpenAI SDK\")\n",
        "print(\"   - Access 250+ models through one interface\")\n",
        "print(\"   - Get detailed analytics and monitoring\")\n",
        "print(\"   - Manage fallbacks and error handling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Reusable chat_with_prompt function defined\n",
            "   ‚Ä¢ This function shows the correct extra_body structure\n",
            "   ‚Ä¢ Use this instead of manually building extra_body\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Define a reusable function for prompt-based chat completions\n",
        "def chat_with_prompt(\n",
        "    client, \n",
        "    prompt_id, \n",
        "    variables=None, \n",
        "    model=\"gpt-4o-mini\",\n",
        "    customer_identifier=None,\n",
        "    metadata=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Make a chat completion using a Keywords AI prompt.\n",
        "    Shows the exact extra_body structure being sent for educational purposes.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenAI client configured for Keywords AI\n",
        "        prompt_id: ID of the prompt to use\n",
        "        variables: Dict of variables to substitute in the prompt\n",
        "        model: Model to use (default: gpt-4o-mini)\n",
        "        customer_identifier: Optional customer identifier for tracking\n",
        "        metadata: Optional metadata dict for analytics\n",
        "    \n",
        "    Returns:\n",
        "        OpenAI ChatCompletion response\n",
        "    \"\"\"\n",
        "    # Build the correct extra_body structure\n",
        "    extra_body = {\n",
        "        \"prompt\": {\n",
        "            \"prompt_id\": prompt_id,\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    if variables:\n",
        "        extra_body[\"prompt\"][\"variables\"] = variables\n",
        "    \n",
        "    if customer_identifier:\n",
        "        extra_body[\"customer_identifier\"] = customer_identifier\n",
        "        \n",
        "    if metadata:\n",
        "        extra_body[\"metadata\"] = metadata\n",
        "    \n",
        "    # Print the structure so users can see exactly what's being sent\n",
        "    print(f\"üîß Keywords AI Extra Body Structure:\")\n",
        "    print(json.dumps(extra_body, indent=2))\n",
        "    print()\n",
        "    \n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \".\",  # Placeholder for OAI SDK, will be replaced by the prompt\n",
        "            }\n",
        "        ],\n",
        "        extra_body=extra_body,\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Reusable chat_with_prompt function defined\")\n",
        "print(\"   ‚Ä¢ This function shows the correct extra_body structure\")\n",
        "print(\"   ‚Ä¢ Use this instead of manually building extra_body\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Try to Use the Prompt (Expected Error)\n",
        "\n",
        "Let's try to use our prompt with the OpenAI client. We expect this to fail because the prompt hasn't been deployed yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Attempting to use the prompt (expecting error)...\n",
            "üîß Keywords AI Extra Body Structure:\n",
            "{\n",
            "  \"prompt\": {\n",
            "    \"prompt_id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "    \"variables\": {\n",
            "      \"customer_inquiry\": \"My order hasn't arrived yet. What should I do?\"\n",
            "    }\n",
            "  },\n",
            "  \"customer_identifier\": \"demo_user_123\",\n",
            "  \"metadata\": {\n",
            "    \"demo\": \"prompt_workflow\"\n",
            "  }\n",
            "}\n",
            "\n",
            "‚ùå Expected error occurred: Error code: 404 - {'error': {'code': 'keywordsai_error', 'message': 'No live version found for the prompt, try deploying a version first!', 'param': None, 'type': 'keywordsai_error'}}\n",
            "üîç Error type: NotFoundError\n",
            "\n",
            "üí° This is expected because:\n",
            "   - The prompt hasn't been deployed yet\n",
            "   - Prompts need to be deployed before they can be used\n",
            "   - We'll deploy it in the next step\n"
          ]
        }
      ],
      "source": [
        "# Try to use the prompt - this should fail because it's not deployed\n",
        "print(\"üß™ Attempting to use the prompt (expecting error)...\")\n",
        "\n",
        "try:\n",
        "    response = chat_with_prompt(\n",
        "        client=openai_client,\n",
        "        prompt_id=PROMPT_ID,\n",
        "        variables={\n",
        "            \"customer_inquiry\": \"My order hasn't arrived yet. What should I do?\"\n",
        "        },\n",
        "        customer_identifier=\"demo_user_123\",\n",
        "        metadata={\"demo\": \"prompt_workflow\"}\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Unexpected success! Response: {response.choices[0].message.content}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Expected error occurred: {str(e)}\")\n",
        "    print(f\"üîç Error type: {type(e).__name__}\")\n",
        "    print(\"\\nüí° This is expected because:\")\n",
        "    print(\"   - The prompt hasn't been deployed yet\")\n",
        "    print(\"   - Prompts need to be deployed before they can be used\")\n",
        "    print(\"   - We'll deploy it in the next step\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Try to Deploy the Prompt (Expected Draft Error)\n",
        "\n",
        "Let's try to deploy our prompt. This might fail if the prompt is still in draft state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Attempting to deploy the prompt (expecting failure)...\n",
            "‚úÖ Unexpected success! Prompt deployed:\n",
            "   Live version: None\n"
          ]
        }
      ],
      "source": [
        "# Try to deploy the prompt - this should fail because we only have draft versions\n",
        "print(\"üöÄ Attempting to deploy the prompt (expecting failure)...\")\n",
        "\n",
        "try:\n",
        "    # Try to deploy the prompt using the actual deploy method\n",
        "    deployed_prompt = await client.adeploy(PROMPT_ID)\n",
        "    print(f\"‚úÖ Unexpected success! Prompt deployed:\")\n",
        "    print(f\"   Live version: {deployed_prompt.live_version.version if deployed_prompt.live_version else 'None'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Expected deployment failure: {str(e)}\")\n",
        "    print(f\"üîç Error type: {type(e).__name__}\")\n",
        "    print(\"\\nüí° This is expected because:\")\n",
        "    print(\"   - We only have draft versions so far\")\n",
        "    print(\"   - Draft versions cannot be deployed\")\n",
        "    print(\"   - We need to commit the draft first (create another version)\")\n",
        "    \n",
        "    # Let's verify the current state\n",
        "    prompt_details = await client.aget(PROMPT_ID)\n",
        "    print(f\"\\nüìã Current prompt state:\")\n",
        "    print(f\"   Name: {prompt_details.name}\")\n",
        "    print(f\"   Total versions: {prompt_details.commit_count}\")\n",
        "    print(f\"   Live version: {prompt_details.live_version}\")\n",
        "    print(\"   ‚Üí No live version means prompt is not deployed yet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Commit Draft Version\n",
        "\n",
        "To make a version deployable, we need to commit the current draft. This creates a readonly version that can be deployed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Creating a new draft version to commit the current one...\n",
            "‚úÖ Created version 2\n",
            "   This should make version 1 readonly and deployable\n",
            "\n",
            "üîç Version 1 status:\n",
            "   Readonly: True\n",
            "   ‚úÖ Ready for deployment\n",
            "\n",
            "üîó Version 2 created (current draft)\n"
          ]
        }
      ],
      "source": [
        "# Create a new draft version that commits the current draft into a readonly version\n",
        "print(\"üì¶ Creating a new draft version to commit the current one...\")\n",
        "\n",
        "# This creates version 2, which makes version 1 readonly and deployable\n",
        "version_data_v2 = PromptVersion(\n",
        "    prompt_version_id=\"version-002\",\n",
        "    description=\"Second version - this commits version 1\",\n",
        "    created_at=datetime.now(timezone.utc),\n",
        "    updated_at=datetime.now(timezone.utc),\n",
        "    version=2,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a friendly customer support assistant. Use a warm, professional tone while solving customer problems efficiently.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"{{customer_inquiry}}\"},\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.8,  # Slightly higher temperature for more creativity\n",
        "    max_tokens=1800,\n",
        "    top_p=0.95,\n",
        "    frequency_penalty=0.1,\n",
        "    presence_penalty=0.1,\n",
        "    variables={\"customer_inquiry\": \"Customer's question or issue\"},\n",
        "    parent_prompt=PROMPT_ID,\n",
        ")\n",
        "\n",
        "version_v2 = await client.acreate_version(PROMPT_ID, version_data_v2)\n",
        "print(f\"‚úÖ Created version {version_v2.version}\")\n",
        "print(f\"   This should make version {VERSION_1} readonly and deployable\")\n",
        "\n",
        "# Check if version 1 is now readonly\n",
        "version_1_check = await client.aget_version(PROMPT_ID, VERSION_1)\n",
        "print(f\"\\nüîç Version {VERSION_1} status:\")\n",
        "print(f\"   Readonly: {version_1_check.readonly}\")\n",
        "print(f\"   {'‚úÖ Ready for deployment' if version_1_check.readonly else '‚ö†Ô∏è  Still draft'}\")\n",
        "\n",
        "VERSION_2 = version_v2.version\n",
        "print(f\"\\nüîó Version {VERSION_2} created (current draft)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Deploying the prompt...\n",
            "‚úÖ Successfully deployed prompt!\n",
            "   Live version: None\n",
            "   Prompt is now ready for use with OpenAI SDK\n",
            "   ‚ö†Ô∏è  No live version found after deployment attempt\n",
            "\n",
            "üí° What deployment does:\n",
            "   - Makes a readonly version the 'live' version\n",
            "   - Enables the prompt for use with OpenAI SDK\n",
            "   - Allows variable substitution in prompt messages\n",
            "   - Provides analytics and monitoring through Keywords AI\n"
          ]
        }
      ],
      "source": [
        "# Deploy the prompt to make it live\n",
        "print(\"üöÄ Deploying the prompt...\")\n",
        "\n",
        "try:\n",
        "    deployed_prompt = await client.adeploy(PROMPT_ID)\n",
        "    print(f\"‚úÖ Successfully deployed prompt!\")\n",
        "    print(f\"   Live version: {deployed_prompt.live_version.version if deployed_prompt.live_version else 'None'}\")\n",
        "    print(f\"   Prompt is now ready for use with OpenAI SDK\")\n",
        "    \n",
        "    # Verify deployment by checking prompt details\n",
        "    deployment_check = await client.aget(PROMPT_ID)\n",
        "    if deployment_check.live_version:\n",
        "        print(f\"   üîç Deployment confirmed - Live version {deployment_check.live_version.version} is active\")\n",
        "        print(f\"   Model: {deployment_check.live_version.model}\")\n",
        "        print(f\"   Temperature: {deployment_check.live_version.temperature}\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  No live version found after deployment attempt\")\n",
        "        \n",
        "except Exception as deploy_error:\n",
        "    print(f\"‚ùå Deployment failed: {str(deploy_error)}\")\n",
        "    print(\"üí° This might happen if:\")\n",
        "    print(\"   - No readonly version is available\")\n",
        "    print(\"   - All versions are still in draft state\") \n",
        "    print(\"   - API deployment endpoint is not available\")\n",
        "\n",
        "print(\"\\nüí° What deployment does:\")\n",
        "print(\"   - Makes a readonly version the 'live' version\")\n",
        "print(\"   - Enables the prompt for use with OpenAI SDK\")\n",
        "print(\"   - Allows variable substitution in prompt messages\")\n",
        "print(\"   - Provides analytics and monitoring through Keywords AI\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Test the Deployed Prompt\n",
        "\n",
        "Now that we have deployed the prompt, let's test it with the OpenAI SDK to verify it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Attempting to use the prompt with OpenAI client...\n",
            "üîß Keywords AI Extra Body Structure:\n",
            "{\n",
            "  \"prompt\": {\n",
            "    \"prompt_id\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "    \"variables\": {\n",
            "      \"customer_inquiry\": \"My order hasn't arrived yet. What should I do?\"\n",
            "    }\n",
            "  },\n",
            "  \"customer_identifier\": \"demo_user_123\",\n",
            "  \"metadata\": {\n",
            "    \"demo\": \"prompt_workflow\",\n",
            "    \"prompt_used\": \"63cc66077f3b4d349ea191d981647981\",\n",
            "    \"version_used\": 1\n",
            "  }\n",
            "}\n",
            "\n",
            "‚úÖ Success! Response received:\n",
            "üìù Content: Hello! How can I assist you today?\n",
            "üîß Model used: gpt-4o-mini-2024-07-18\n",
            "üìä Tokens used: 17\n",
            "\n",
            "üí° What happened:\n",
            "   - Request went through Keywords AI gateway\n",
            "   - Used the committed prompt version\n",
            "   - Got analytics and monitoring\n",
            "   - Response logged in Keywords AI dashboard\n"
          ]
        }
      ],
      "source": [
        "# Now try to use the prompt with OpenAI client\n",
        "print(\"üöÄ Attempting to use the prompt with OpenAI client...\")\n",
        "\n",
        "try:\n",
        "    # Use the reusable function to make a proper prompt-based call\n",
        "    response = chat_with_prompt(\n",
        "        client=openai_client,\n",
        "        prompt_id=PROMPT_ID,\n",
        "        variables={\n",
        "            \"customer_inquiry\": \"My order hasn't arrived yet. What should I do?\"\n",
        "        },\n",
        "        customer_identifier=\"demo_user_123\",\n",
        "        metadata={\n",
        "            \"demo\": \"prompt_workflow\",\n",
        "            \"prompt_used\": PROMPT_ID,\n",
        "            \"version_used\": VERSION_1\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Success! Response received:\")\n",
        "    print(f\"üìù Content: {response.choices[0].message.content}\")\n",
        "    print(f\"üîß Model used: {response.model}\")\n",
        "    print(f\"üìä Tokens used: {response.usage.total_tokens if response.usage else 'N/A'}\")\n",
        "    \n",
        "    print(\"\\nüí° What happened:\")\n",
        "    print(\"   - Request went through Keywords AI gateway\")\n",
        "    print(\"   - Used the committed prompt version\")\n",
        "    print(\"   - Got analytics and monitoring\")\n",
        "    print(\"   - Response logged in Keywords AI dashboard\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {str(e)}\")\n",
        "    print(f\"üîç Error type: {type(e).__name__}\")\n",
        "    print(\"\\nüí° If this failed, it might be because:\")\n",
        "    print(\"   - Prompt deployment is not fully implemented yet\")\n",
        "    print(\"   - Need to manually deploy in the UI\")\n",
        "    print(\"   - API endpoint differences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Iterate - Create a New Version\n",
        "\n",
        "Let's create a new version with different settings to demonstrate iteration and A/B testing capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating version 3 with a different approach...\n",
            "‚úÖ Created version 3\n",
            "   Model: gpt-4o-mini\n",
            "   Temperature: 0.3 (lower for more focused responses)\n",
            "   Max tokens: 1000 (shorter responses)\n",
            "\n",
            "üîç Version 2 status:\n",
            "   Readonly: True\n",
            "   ‚úÖ Ready for deployment\n",
            "\n",
            "üîó Version 3 created (current draft)\n",
            "\n",
            "üìã All versions for prompt 63cc66077f3b4d349ea191d981647981:\n",
            "   - Version 3: Concise and direct customer support version (draft)\n",
            "     Temperature: 0.3, Max tokens: 1000\n",
            "   - Version 2: Second version - this commits version 1 (readonly)\n",
            "     Temperature: 0.8, Max tokens: 1800\n",
            "   - Version 1: Updated: Professional customer support with improved clarity (readonly)\n",
            "     Temperature: 0.6, Max tokens: 2500\n"
          ]
        }
      ],
      "source": [
        "# Create a new version with different approach - more concise and direct\n",
        "print(\"üîß Creating version 3 with a different approach...\")\n",
        "\n",
        "version_data_v3 = PromptVersion(\n",
        "    prompt_version_id=\"version-003\",\n",
        "    description=\"Concise and direct customer support version\",\n",
        "    created_at=datetime.now(timezone.utc),\n",
        "    updated_at=datetime.now(timezone.utc),\n",
        "    version=3,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a concise customer support assistant. Provide direct, actionable solutions quickly. Be helpful but brief.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"{{customer_inquiry}}\"},\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.3,  # Lower temperature for more focused responses\n",
        "    max_tokens=1000,  # Shorter responses\n",
        "    top_p=0.8,\n",
        "    frequency_penalty=0.2,\n",
        "    presence_penalty=0.0,\n",
        "    variables={\"customer_inquiry\": \"Customer's question or issue\"},\n",
        "    parent_prompt=PROMPT_ID,\n",
        ")\n",
        "\n",
        "version_v3 = await client.acreate_version(PROMPT_ID, version_data_v3)\n",
        "print(f\"‚úÖ Created version {version_v3.version}\")\n",
        "print(f\"   Model: {version_v3.model}\")\n",
        "print(f\"   Temperature: {version_v3.temperature} (lower for more focused responses)\")\n",
        "print(f\"   Max tokens: {version_v3.max_tokens} (shorter responses)\")\n",
        "\n",
        "# Check version 2 status (should now be readonly)\n",
        "version_2_check = await client.aget_version(PROMPT_ID, VERSION_2)\n",
        "print(f\"\\nüîç Version {VERSION_2} status:\")\n",
        "print(f\"   Readonly: {version_2_check.readonly}\")\n",
        "print(f\"   {'‚úÖ Ready for deployment' if version_2_check.readonly else '‚ö†Ô∏è  Still draft'}\")\n",
        "\n",
        "VERSION_3 = version_v3.version\n",
        "print(f\"\\nüîó Version {VERSION_3} created (current draft)\")\n",
        "\n",
        "# List all versions\n",
        "print(f\"\\nüìã All versions for prompt {PROMPT_ID}:\")\n",
        "versions_list = await client.alist_versions(PROMPT_ID)\n",
        "for v in versions_list.results:\n",
        "    status = \"readonly\" if v.readonly else \"draft\"\n",
        "    print(f\"   - Version {v.version}: {v.description} ({status})\")\n",
        "    print(f\"     Temperature: {v.temperature}, Max tokens: {v.max_tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Deploy New Version\n",
        "\n",
        "Let's test the different versions to see how they perform with the same input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing different versions with the same input:\n",
            "üìù Test message: My order hasn't arrived yet and I need it urgently for a meeting tomorrow. What can you do to help?\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üîç Testing Version 1 (Professional, temp=0.7):\n",
            "‚úÖ Version 1 Response: I‚Äôm sorry to hear that your order hasn‚Äôt arrived yet, especially since you need it for an important meeting. To assist you better, could you please provide me with your order number? I‚Äôll check the st...\n",
            "üìä Tokens: 115\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîç Testing Version 2 (Friendly, temp=0.8):\n",
            "‚úÖ Version 2 Response: I completely understand how important it is to receive your order on time, especially with your meeting coming up tomorrow. Let‚Äôs get this sorted out for you!\n",
            "\n",
            "Could you please provide me with your or...\n",
            "üìä Tokens: 121\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîç Testing Version 3 (Concise, temp=0.3):\n",
            "‚úÖ Version 3 Response: Please provide your order number, and I can check the status for you. If it's delayed, I can assist with expediting the shipping if possible....\n",
            "üìä Tokens: 83\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üí° Key Observations:\n",
            "   - Different versions produce different response styles\n",
            "   - Temperature affects creativity vs consistency\n",
            "   - Max tokens controls response length\n",
            "   - All requests are logged in Keywords AI for analysis\n",
            "   - You can A/B test different versions in production\n"
          ]
        }
      ],
      "source": [
        "# Compare different versions with the same input\n",
        "test_message = \"My order hasn't arrived yet and I need it urgently for a meeting tomorrow. What can you do to help?\"\n",
        "\n",
        "print(\"üß™ Testing different versions with the same input:\")\n",
        "print(f\"üìù Test message: {test_message}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Test Version 1 (Professional tone, temp 0.7)\n",
        "print(\"\\nüîç Testing Version 1 (Professional, temp=0.7):\")\n",
        "try:\n",
        "    response_v1 = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a professional customer support assistant. Always be helpful, polite, and provide clear solutions to customer problems.\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": test_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=2048,\n",
        "        extra_body={\n",
        "            \"customer_identifier\": \"demo_user_123\",\n",
        "            \"metadata\": {\"version_test\": \"v1\", \"demo\": \"comparison\"}\n",
        "        }\n",
        "    )\n",
        "    print(f\"‚úÖ Version 1 Response: {response_v1.choices[0].message.content[:200]}...\")\n",
        "    print(f\"üìä Tokens: {response_v1.usage.total_tokens if response_v1.usage else 'N/A'}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Version 1 Error: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "# Test Version 2 (Friendly tone, temp 0.8) \n",
        "print(\"\\nüîç Testing Version 2 (Friendly, temp=0.8):\")\n",
        "try:\n",
        "    response_v2 = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a friendly customer support assistant. Use a warm, professional tone while solving customer problems efficiently.\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": test_message}\n",
        "        ],\n",
        "        temperature=0.8,\n",
        "        max_tokens=1800,\n",
        "        extra_body={\n",
        "            \"customer_identifier\": \"demo_user_123\",\n",
        "            \"metadata\": {\"version_test\": \"v2\", \"demo\": \"comparison\"}\n",
        "        }\n",
        "    )\n",
        "    print(f\"‚úÖ Version 2 Response: {response_v2.choices[0].message.content[:200]}...\")\n",
        "    print(f\"üìä Tokens: {response_v2.usage.total_tokens if response_v2.usage else 'N/A'}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Version 2 Error: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "# Test Version 3 (Concise, temp 0.3)\n",
        "print(\"\\nüîç Testing Version 3 (Concise, temp=0.3):\")\n",
        "try:\n",
        "    response_v3 = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a concise customer support assistant. Provide direct, actionable solutions quickly. Be helpful but brief.\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": test_message}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=1000,\n",
        "        extra_body={\n",
        "            \"customer_identifier\": \"demo_user_123\",\n",
        "            \"metadata\": {\"version_test\": \"v3\", \"demo\": \"comparison\"}\n",
        "        }\n",
        "    )\n",
        "    print(f\"‚úÖ Version 3 Response: {response_v3.choices[0].message.content[:200]}...\")\n",
        "    print(f\"üìä Tokens: {response_v3.usage.total_tokens if response_v3.usage else 'N/A'}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Version 3 Error: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nüí° Key Observations:\")\n",
        "print(\"   - Different versions produce different response styles\")\n",
        "print(\"   - Temperature affects creativity vs consistency\")\n",
        "print(\"   - Max tokens controls response length\")\n",
        "print(\"   - All requests are logged in Keywords AI for analysis\")\n",
        "print(\"   - You can A/B test different versions in production\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
